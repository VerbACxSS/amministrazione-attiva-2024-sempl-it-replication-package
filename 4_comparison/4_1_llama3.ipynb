{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6Cz36RKs9W9"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1yzKv4WBWDn"
   },
   "source": [
    "## Dependencies and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSDf1n7iIJo_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7ODyTS9rX3s"
   },
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OknJILjcrX3t"
   },
   "outputs": [],
   "source": [
    "class AbstractModel(ABC):\n",
    "    PROMPT = \"Sei un dipendente pubblico che deve riscrivere dei documenti istituzionali italiani per renderli semplici e comprensibili per i cittadini. Ti verrà fornito un documento pubblico e il tuo compito sarà quello di riscriverlo applicando regole di semplificazione senza però modificare il significato del documento originale. Ad esempio potresti rendere le frasi più brevi, eliminare le perifrasi, esplicitare sempre il soggetto, utilizzare parole più semplici, trasformare i verbi passivi in verbi di forma attiva, spostare le frasi parentetiche alla fine del periodo.\"\n",
    "\n",
    "    def __init__(self, hugging_face_model_id: str, torch_dtype=torch.bfloat16, quantization_config=None):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} for inference\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_id, token=os.getenv(\"HF_TOKEN\"))\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(hugging_face_model_id,\n",
    "                                                          trust_remote_code=True,\n",
    "                                                          device_map=self.device,\n",
    "                                                          torch_dtype=torch_dtype,\n",
    "                                                          token=os.getenv(\"HF_TOKEN\"),\n",
    "                                                          quantization_config=quantization_config).eval()\n",
    "        print(\"Model loaded\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_prompt(self, _text_to_simplify):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, _decoded):\n",
    "        pass\n",
    "\n",
    "    def predict(self, _texts_to_simplify):\n",
    "        prompts = [self.build_prompt(_text) for _text in _texts_to_simplify]\n",
    "        outputs = []\n",
    "        for prompt in tqdm(prompts):\n",
    "            x = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "            y = self.model.generate(x, max_new_tokens=512, temperature=0.2, top_p=0.1, do_sample=True, eos_token_id=self.tokenizer.eos_token_id, pad_token_id=self.tokenizer.eos_token_id)\n",
    "            decoded = self.tokenizer.batch_decode(y, skip_special_tokens=False)\n",
    "            decoded = [self.decode(d) for d in decoded]\n",
    "            outputs.extend(decoded)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqnSFlM07ZiK"
   },
   "outputs": [],
   "source": [
    "class Llama3(AbstractModel):\n",
    "    HUGGING_FACE_MODEL_ID = \"DeepMount00/Llama-3-8b-Ita\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(Llama3.HUGGING_FACE_MODEL_ID, torch.bfloat16)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def build_prompt(self, _text_to_simplify):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": Llama3.PROMPT},\n",
    "            {\"role\": \"user\", \"content\": _text_to_simplify},\n",
    "        ]\n",
    "        return self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "    def decode(self, _decoded):\n",
    "        return _decoded.split('<|start_header_id|>assistant<|end_header_id|>\\n\\n')[-1].split('<|eot_id|>')[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvcgTTzMtJre"
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6KiE9ouIY5A"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./texts/original.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7uPiyDV3Qw6"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTQRDmt13SIA"
   },
   "outputs": [],
   "source": [
    "model = Llama3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q08YpCm6tP_9"
   },
   "source": [
    "## Random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12809,
     "status": "ok",
     "timestamp": 1715029321528,
     "user": {
      "displayName": "Marco Russodivito",
      "userId": "05013084347772678948"
     },
     "user_tz": -120
    },
    "id": "-DgTSrZbJ8iD",
    "outputId": "9c2ec48c-124e-44d2-f374-f60baf085cf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.8a362e755d2faf8cec2bf98850ce2216023d178a.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original:  I procedimenti amministrativi oggetto dei Servizi di Polizia Locale Reparto Operativo sono indicati nel prospetto di seguito riportato. \n",
      "\n",
      "model:  I Servizi di Polizia Locale Reparto Operativo gestiscono i seguenti procedimenti amministrativi: \n",
      "\n",
      "- Verifica dei documenti\n",
      "\n",
      "- Controllo delle attività\n",
      "\n",
      "- Verifica delle informazioni\n",
      "\n",
      "- Verifica delle attività\n",
      "\n",
      "- Verifica delle informazioni.\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in df.sample(1).to_dict(orient='records'):\n",
    "  output = model.predict([s['original_text']])[0]\n",
    "  print(\"\\noriginal: \", s['original_text'])\n",
    "  print(\"\\nmodel: \", output)\n",
    "  print('----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q76hte_4tUUQ"
   },
   "source": [
    "## Run all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5067466,
     "status": "ok",
     "timestamp": 1715034630467,
     "user": {
      "displayName": "Marco Russodivito",
      "userId": "05013084347772678948"
     },
     "user_tz": -120
    },
    "id": "NbDDDdVLKxg7",
    "outputId": "601d2db9-7fea-400b-c087-80aec155cb47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [1:24:27<00:00,  8.19s/it]\n"
     ]
    }
   ],
   "source": [
    "df['simplified_text'] = model.predict(df['original_text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "742Uidc_tYe5"
   },
   "source": [
    "## Save simplified datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcNZ9y7JLDH2"
   },
   "outputs": [],
   "source": [
    "df.to_csv('./texts/llama3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
